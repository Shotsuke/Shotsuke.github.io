---
title: "LLM-Align: Utilizing Large Language Models for Entity Alignment in Knowledge Graphs"
date: 2025-09-23 14:40:00
categories: "Paper Notes"
tags:
    - Note
    - Paper Reading
    - LLM
    - Entity Alignment
excerpt: LLM 来完全决策 EA 任务。
---

**原文：**[LLM-Align: Utilizing Large Language Models for Entity Alignment in Knowledge Graphs](https://arxiv.org/abs/2412.04690)

# 1. EA 任务定义

给定两个知识图谱 $G = (E, R, A, L, T\_{att}, T\_{rel})$ 和 $G' = (E', R', A', L', T'\_{att}, T'\_{rel})$（实体集、关系集、属性集、字面量集（属性值集），属性三元组集、关系三元组集）。为源图谱 $G$ 中的每一个实体 $e \in E$，在目标图谱 $G'$ 的实体集合 $E'$ 中，找到那个与它指代现实世界中同一对象的等价实体 $e'$。

# 2. 文章工作

## Stage 1: 候选实体检索

输出一个候选实体列表，可以使用 **任何现有的EA模型** 来完成这一步。核心思想是选择一个计算成本极低的方法，快速为每个源实体筛选出一个可能包含真实对齐实体列表。

## Stage 2: 基于属性的推理 (Attribute-based Reasoning)

LLM 介入的第一轮精细过滤。此阶段只关注实体的属性（Attribute）信息。

将属性三元组转换为自然的描述性句子，要求LLM忽略关系信息，只专注于比较属性，LLM会利用其常识（如单位换算、日期格式识别）进行判断。

## Stage 3: 基于关系的推理 (Relation-based Reasoning)

对于通过属性检查的候选对，LLM 将结合其关系（Relation）上下文进行深度语义推理。

将关系三元组转换为描述上下文和角色的句子，要求LLM综合评估这些关系所描述的生活经历、职业轨迹、社会关联等是否一致。

## Stage 4: 多轮投票机制 (Multi-round Voting Machanism)

LLM在处理长文本时，对出现在开头和结尾的信息记忆和理解更好，而容易忽略中间部分的信息。

因此，通过随机打乱顺序，确保每个候选实体都有机会出现在提示的开头、中间和尾部，从而公平地被LLM评估。多次随机后统计最终结果，超过半数被选中的实体作为最终答案。

# 3. 结果

**表1：DBP15K 数据集的统计信息**

| Dataset | Lang.    |  Entity | Rel. | Attr. | Rel.triples | Attr.triples |
| :------ | :------- | ------: | ---: | ----: | ----------: | -----------: |
| ZH-EN   | Chinese  |  66,469 | 2830 |  8113 |     153,929 |      379,684 |
|         | English  |  98,125 | 2317 |  7173 |     237,674 |      567,755 |
| JP-EN   | Japanese |  65,744 | 2043 |  5882 |     164,373 |      354,619 |
|         | English  |  95,680 | 2096 |  6066 |     233,319 |      497,230 |
| FR-EN   | French   |  66,858 | 1379 |  4547 |     192,191 |      528,665 |
|         | English  | 105,889 | 2209 |  6422 |     278,590 |      576,540 |

**表2：DBP15K 数据集上的总体结果**

| Model                         | ZH-EN     |         | JA-EN     |         | FR-EN     |         |
| ----------------------------- | --------- | ------- | --------- | ------- | --------- | ------- |
|                               | Hits@1    | Hits@10 | Hits@1    | Hits@10 | Hits@1    | Hits@10 |
| GCN-Align                     | 0.420     | 0.790   | 0.445     | 0.815   | 0.432     | 0.812   |
| TEA                           | 0.941     | 0.983   | 0.941     | 0.979   | 0.987     | 0.996   |
| BERT-INT                      | 0.968     | 0.990   | 0.964     | 0.991   | 0.992     | 0.998   |
| HMAN                          | 0.871     | 0.987   | 0.935     | 0.994   | 0.973     | 0.998   |
| AttrGNN                       | 0.796     | 0.929   | 0.783     | 0.921   | 0.919     | 0.978   |
| DERA                          | 0.968     | 0.994   | 0.967     | 0.992   | 0.989     | 0.999   |
| DERA-R                        | 0.955     | 0.992   | 0.950     | 0.989   | 0.991     | 1.000   |
| LLMEA                         | 0.898     | 0.923   | 0.911     | 0.946   | 0.957     | 0.977   |
| ChatEA                        | -         | -       | -         | -       | 0.990     | 1.000   |
| LLM-Align (GCN-Align-Qwen14B) | 0.749     | -       | 0.785     | -       | 0.805     | -       |
| LLM-Align (GCN-Align-Qwen32B) | 0.769     | -       | 0.792     | -       | 0.812     | -       |
| LLM-Align (DERA-R-Qwen14B)    | 0.978     | -       | 0.957     | -       | 0.992     | -       |
| LLM-Align (DERA-R-Qwen32B)    | **0.983** | -       | **0.976** | -       | **0.995** | -       |

**表3：消融实验结果**

| AR  | RR  | MV  | ZH-EN |           | JA-EN |           | FR-EN |           |
| --- | --- | --- | ----- | --------- | ----- | --------- | ----- | --------- |
|     |     |     | 14B   | 32B       | 14B   | 32B       | 14B   | 32B       |
| ✓   | ✓   | ✓   | 0.978 | **0.983** | 0.957 | **0.976** | 0.992 | **0.995** |
| ✗   | ✓   | ✓   | 0.817 | 0.872     | 0.804 | 0.865     | 0.852 | 0.973     |
| ✓   | ✗   | ✓   | 0.952 | 0.980     | 0.938 | 0.973     | 0.990 | 0.994     |
| ✓   | ✓   | ✗   | 0.918 | 0.964     | 0.926 | 0.968     | 0.954 | 0.986     |
| ✗   | ✓   | ✗   | 0.731 | 0.813     | 0.722 | 0.823     | 0.794 | 0.933     |
| ✓   | ✗   | ✗   | 0.909 | 0.971     | 0.914 | 0.952     | 0.947 | 0.981     |